{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MonodepthModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Model Architecture\n",
    "    Encoder:\n",
    "        Layer | Kernal size | stride | input-output | downsample input/output | input\n",
    "        conv1       7            2       3/32               1/2                 left image\n",
    "        conv1b      7            1       32/32              2/2                 conv1\n",
    "        conv2       5            2       32/64              2/4                 conv1b\n",
    "        conv2b      5            1       64/64              4/4                 conv2\n",
    "        conv3       3            2       64/128             4/8                 conv2b\n",
    "        conv3b      3            1       128/128            8/8                 conv3\n",
    "        conv4       3            2       128/256            8/16                conv3b\n",
    "        conv4b      3            1       256/256            16/16               conv4\n",
    "        conv5       3            2       256/512            16/32               conv4b\n",
    "        conv5b      3            1       512/512            32/32               conv5\n",
    "        conv6       3            2       512/512            32/64               conv5b\n",
    "        conv6b      3            1       512/512            64/64               conv6\n",
    "        conv7       3            2       512/512            64/128              conv6b\n",
    "        conv7b      3            1       512/512            128/128             conv7\n",
    "    Deoder:\n",
    "        upconv7     3            2       512/512            128/64              conv7b\n",
    "        iconv7      3            1       1024/512           64/64               upconv7+conv6b\n",
    "        upconv6     3            2       512/512            64/32               iconv7\n",
    "        iconv6      3            1       1024/512           32/32               upconv6+conv5b\n",
    "        upconv5     3            2       512/256            32/16               iconv6\n",
    "        iconv5      3            1       512/256            16/16               upconv5+conv4b\n",
    "        upconv4     3            2       256/128            16/8                iconv5\n",
    "        iconv4      3            1       256/128            8/8                 upconv4+conv3b\n",
    "        disp4       3            1       128/2              8/8                 iconv4\n",
    "        upconv3     3            2       128/64             8/4                 iconv4\n",
    "        iconv3      3            1       130/64             4/4                 upconv3+conv2b+disp4*\n",
    "        disp3       3            1       64/2               4/4                 iconv3\n",
    "        upconv2     3            2       64/32              4/2                 iconv3\n",
    "        iconv2      3            1       66/32              2/2                 upconv2+conv1b+disp3*\n",
    "        disp2       3            1       32/2               2/2                 iconv2\n",
    "        upconv1     3            2       32/16              2/1                 iconv2\n",
    "        iconv1      3            1       18/16              1/1                 upconv1+disp2*\n",
    "        disp1       3            1       16/2               1/1                 iconv1\n",
    "        \n",
    "    ∗ is a 2× upsampling of the layer\n",
    "    \n",
    "    torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode:='zeros')\n",
    "    torch.nn.BatchNorm2d(num_features, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "    Padding size calculation [image\\featuremap width\\height = W, kernal size = K, strides = S, padding = P] \n",
    "    output_layer height/width = [(W-K+2P)/S]+1\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):    \n",
    "        super(MonodepthModel,self).__init__()\n",
    "        \n",
    "        #Encoder                                                   # input 512*256*3 \n",
    "        self.conv1 = nn.Conv2d(3,32,7,stride=2,padding=3)          # 256*128*32\n",
    "        self.conv1b = nn.BatchNorm2d(32, affine = False)           # 256*128*32\n",
    "         \n",
    "        self.conv2 = nn.Conv2d(32,64,5,stride=2,padding=2)         # 128*64*64\n",
    "        self.conv2b = nn.BatchNorm2d(64, affine = False)           # 128*64*64\n",
    "\n",
    "        self.conv3 = nn.Conv2d(64,128,3,stride=2,padding=1)        # 64*32*128\n",
    "        self.conv3b = nn.BatchNorm2d(128, affine = False)          # 64*32*128\n",
    "        \n",
    "        self.conv4 = nn.Conv2d(128,256,3,stride=2,padding=1)       # 32*16*256\n",
    "        self.conv4b = nn.BatchNorm2d(256, affine = False)          # 32*16*256\n",
    "\n",
    "        self.conv5 = nn.Conv2d(256,512,3,stride=2,padding=1)       # 16*8*512\n",
    "        self.conv5b = nn.BatchNorm2d(512, affine = False)          # 16*8*512\n",
    "        \n",
    "        self.conv6 = nn.Conv2d(512,512,3,stride=2,padding=1)       # 8*4*512\n",
    "        self.conv6b = nn.BatchNorm2d(512, affine = False)          # 8*4*512\n",
    "        \n",
    "        self.conv7 = nn.Conv2d(512,512,3,stride=2,padding=1)       # 4*2*512\n",
    "        self.conv7b = nn.BatchNorm2d(512, affine = False)          # 4*2*512\n",
    "        \n",
    "        #Decoder\n",
    "               \n",
    "        self.upconv7 = nn.Conv2d(512,512,3,stride=1,padding=1)    # 8*4*512 | input = 8*4*512 -> upsampling F.interpolate on conv7b\n",
    "        self.iconv7 = nn.Conv2d(512+512,512,3,stride=1,padding=1) # 8*4*512 -> aditional input layers because of skip connection torch.cat\n",
    "    \n",
    "        self.upconv6 = nn.Conv2d(512,512,3,stride=1,padding=1)    # 16*8*512 -> upsampling F.interpolate\n",
    "        self.iconv6 = nn.Conv2d(512+512,512,3,stride=1,padding=1) # 16*8*512\n",
    "                \n",
    "        self.upconv5 = nn.Conv2d(512,256,3,stride=1,padding=1)    # 32*16*256 \n",
    "        self.iconv5 = nn.Conv2d(256+256,256,3,stride=1,padding=1) # 32*16*256\n",
    "        \n",
    "        self.upconv4 = nn.Conv2d(256,128,3,stride=1,padding=1)    # 64*32*128 \n",
    "        self.iconv4 = nn.Conv2d(128+128,128,3,stride=1,padding=1) # 64*32*128\n",
    "        self.disp4 = nn.Conv2d(128,2,3,stride=1,padding=1)        # 64*32*2\n",
    "        \n",
    "        self.upconv3 = nn.Conv2d(128,64,3,stride=1,padding=1)    # 128*64*64 \n",
    "        self.iconv3 = nn.Conv2d(130,64,3,stride=1,padding=1)     # 128*64*64   concat [upconv3+conv2b+disp4*]\n",
    "        self.disp3 = nn.Conv2d(64,2,3,stride=1,padding=1)        # 128*64*2\n",
    "\n",
    "        self.upconv2 = nn.Conv2d(64,32,3,stride=1,padding=1)    # 256*128*32 \n",
    "        self.iconv2 = nn.Conv2d(66,32,3,stride=1,padding=1)     # 256*128*32  \n",
    "        self.disp2 = nn.Conv2d(32,2,3,stride=1,padding=1)       # 256*128*2\n",
    "               \n",
    "        self.upconv1 = nn.Conv2d(32,16,3,stride=1,padding=1)    # 512*256*16 \n",
    "        self.iconv1 = nn.Conv2d(18,16,3,stride=1,padding=1)     # 512*256*16   \n",
    "        self.disp1 = nn.Conv2d(16,2,3,stride=1,padding=1)       # 512*256*2\n",
    "        \n",
    "    def forward(self,x):\n",
    "        \n",
    "        # Encoder\n",
    "        x_01 = F.elu(self.conv1b(self.conv1(x)))\n",
    "        x_12 = F.elu(self.conv2b(self.conv2(x_01)))\n",
    "        x_23 = F.elu(self.conv3b(self.conv3(x_12)))\n",
    "        x_34 = F.elu(self.conv4b(self.conv4(x_23)))\n",
    "        x_45 = F.elu(self.conv5b(self.conv5(x_34)))\n",
    "        x_56 = F.elu(self.conv6b(self.conv6(x_45)))\n",
    "        x_67 = F.elu(self.conv7b(self.conv7(x_56)))\n",
    "        \n",
    "        # Decoder\n",
    "        X_77 = F.elu(self.upconv7(F.interpolate(x_67, scale_factor=2, mode='nearest')))\n",
    "        x_76 = F.elu(self.iconv7(torch.cat((x_77,x_56),1)))\n",
    "        \n",
    "        X_66 = F.elu(self.upconv6(F.interpolate(x_76, scale_factor=2, mode='nearest')))\n",
    "        x_65 = F.elu(self.iconv6(torch.cat((x_66,x_45),1)))\n",
    "        \n",
    "        X_55 = F.elu(self.upconv5(F.interpolate(x_65, scale_factor=2, mode='nearest')))\n",
    "        x_54 = F.elu(self.iconv5(torch.cat((x_55,x_34),1)))\n",
    "        \n",
    "        x_44 = F.elu(self.upconv4(F.interpolate(x_54, scale_factor=2, mode='nearest')))\n",
    "        x_43 = F.elu(self.iconv4(torch.cat((x_44,x_23),1)))\n",
    "        x_43_d = F.elu(self.disp4(x_43))\n",
    "        \n",
    "        x_33 = F.elu(self.upconv3(F.interpolate(x_43, scale_factor=2, mode='nearest')))\n",
    "        x_32 = F.elu(self.iconv3(torch.cat((x_33,x_12,F.interpolate(x_43_d, scale_factor=2, mode='nearest')),1)))\n",
    "        x_32_d = F.elu(self.disp3(x_32))\n",
    "        \n",
    "        x_22 = F.elu(self.upconv2(F.interpolate(x_32, scale_factor=2, mode='nearest')))\n",
    "        x_21 = F.elu(self.iconv2(torch.cat((x_22,x_01,F.interpolate(x_32_d, scale_factor=2, mode='nearest')),1)))      \n",
    "        x_21_d = F.elu(self.disp2(x_21))\n",
    "                \n",
    "        x_11 = F.elu(self.upconv1(F.interpolate(x_21, scale_factor=2, mode='nearest')))\n",
    "        x_10 = F.elu(self.iconv1(torch.cat((x_11,F.interpolate(x_21_d, scale_factor=2, mode='nearest')),1)))\n",
    "        x_10_d = F.elu(self.disp1(x_10))\n",
    "        \n",
    "        return [x_10_d, x_21_d, x_32_d, x_43_d]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
